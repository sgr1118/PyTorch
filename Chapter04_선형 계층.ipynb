{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOM6bs/o/c74RKSVtiSZd6o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 4.2 행렬 곱"],"metadata":{"id":"3h9I5rQfYy53"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"jLcN51K7H1zb","executionInfo":{"status":"ok","timestamp":1672642715939,"user_tz":-540,"elapsed":2830,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"outputs":[],"source":["import torch"]},{"cell_type":"code","source":["# x, y 토치 텐서 만들기\n","\n","x = torch.FloatTensor([[1, 2],\n","                      [3, 4],\n","                      [5, 6]])\n","y = torch.FloatTensor([[1, 2],\n","                      [1, 2]])\n","\n","print(x.size(), y.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m_BRAwDYY-7E","executionInfo":{"status":"ok","timestamp":1671537484190,"user_tz":-540,"elapsed":9,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"c88224d5-8b00-4870-a37c-6af3f27f7dfd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 2]) torch.Size([2, 2])\n"]}]},{"cell_type":"code","source":["# x, y 토치 곱 연산\n","\n","z = torch.matmul(x, y)\n","print(z.size())\n","\n","# 기대한 값과 일치"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BfhpEtmsZfI0","executionInfo":{"status":"ok","timestamp":1671537533959,"user_tz":-540,"elapsed":7,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"d2f939dc-19b5-464e-d973-e337e1d154ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 2])\n"]}]},{"cell_type":"markdown","source":["### 1. 배치 행렬 곱\n","\n","딥러닝을 수행할 때 보통 여러 샘플을 동시에 병렬 계산한다. 따라서 행렬 솝 연산의 경우에도 여러 곱셈을 동시에 진행할 수 있어야 한다."],"metadata":{"id":"vW9zTOE8Z0UQ"}},{"cell_type":"code","source":["# bmm 함수 사용\n","\n","x = torch.FloatTensor(3,3,2)\n","y = torch.FloatTensor(3,2,3)\n","\n","z = torch.bmm(x, y)\n","print(z.size()) \n","\n","# torch.Size([3, 3, 3]) 3 x 3 크기의 행렬이 3개 만들어 진다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xAaBrvKJZ-LG","executionInfo":{"status":"ok","timestamp":1671537948416,"user_tz":-540,"elapsed":409,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"4a589e50-e003-414c-b56d-ddfd4182d2c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 3, 3])\n"]}]},{"cell_type":"markdown","source":["## 4.4 선형 계층"],"metadata":{"id":"D-16RHF5bhIK"}},{"cell_type":"markdown","source":["### 1. 직접 구현하기\n","\n","선형 계층은 행렬 곱 연산과 브로드캐스팅 덧셈 연산으로 이루어져 있습니다. 선형 계층의 파라미터 행렬 W가 행렬 곱 연산에 활용될 것이고, 파라미터 벡터 b가 브로드캐스팅 것셈 연산에 활용될 것입니다. 앞에서 배운 파이토치 연산 방법이므로 그대로 다시 구현해보록 한다."],"metadata":{"id":"77OSVnthcaEm"}},{"cell_type":"code","source":["# W, b 만들기\n","\n","W = torch.FloatTensor([[1,2],\n","                       [3,4],\n","                       [5,6]]) # 3,2 크기의 행렬\n","\n","b = torch.FloatTensor([2,2]) # 2개의 요소를 갖는 벡터 b"],"metadata":{"id":"a9Xe7AVZbkOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 선형 계층 함수를 구성하기\n","\n","def linear(x, W, b):\n","    y = torch.matmul(x, W) + b\n","\n","    return y"],"metadata":{"id":"Uy3pJlq4dCbr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 선형 계층 계산\n","\n","x = torch.FloatTensor(4,3)\n","\n","y = linear(x, W, b)\n","print(y.size())\n","\n","# 하지만 이 방법은 파이토치 입장에서 제대로 된 계층으로 취급되지 않습니다.\n","# 제대로 계층을 만드는 방법도 살펴본다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QksH-eMYdTmD","executionInfo":{"status":"ok","timestamp":1671538536300,"user_tz":-540,"elapsed":312,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"2b9731a7-e5b5-48f6-982c-dc61ed3413f3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 2])\n"]}]},{"cell_type":"markdown","source":["### 2. torch.nn.Module 클래스 상속 받기\n","\n","토치에는 nn(neural networks)패키지가 있고 내부에는 미리 정의된 많은 신경망들이 있다. 그리고 그 신경망들은 torch.nn.Module이라는 추상 클래스를 상속받아 정의되어 있다. 바로 이 추상 클래스를 상속받아 선형 계층을 구현할 수 있다."],"metadata":{"id":"23d50XMPd3ua"}},{"cell_type":"code","source":["import torch.nn as nn"],"metadata":{"id":"810QURSYeOqE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 그리고 nn.Module을 상속받은 MyLunear라는 클래스를 정의한다. nn.Module을 상속받은 클래스는 보통 2개의 메서드, __init__과 forward를 오버라이드(override)합니다.\n","\n","- __init__ 함수는 계층 내부에서 필요한 변수를 미리 선언하고 있으며 심지어 또 다른 계층(nn.Module을 상속받은 클래스의 객체)을 소유하도록 할 수도 있다. forward 함수는 계층을 통과하는데 필요한 계산 수행을 한다.\n","\n"],"metadata":{"id":"h27zffxEeTjj"}},{"cell_type":"code","source":["class MyLinear(nn.Module):\n","\n","    def __init__(self, input_dim=3, output_dim=2):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        super().__init__()\n","\n","        self.W = torch.FloatTensor(input_dim, output_dim)\n","        self.b = torch.FloatTensor(output_dim)\n","\n","    # You should override 'forward' method to implement detail.\n","    # The input argument and outputs can be desigend as you wish\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, imput_dim)\n","        y = torch.matmul(x, self.W) + self.b\n","        # |y| = (batch_size, imput_dim) * (input_dim, output_dim) = (batch_size, output_dim)\n","\n","        return y"],"metadata":{"id":"9JIIT5uaeSFh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- forward 함수 내부에 계산되고 있는 텐서들의 모양을 주석으로 적어놓은 것을 볼 수 있다. 이렇게 주석을 통해 텐서의 모양을 미리 적어놓으면 추후에 코드를 파악하거나 디버깅을 할 때 훨씬 편리합니다. 앞에서와 같이 정의된 클래스를 이제 생성하여 사용할 수 있습니다."],"metadata":{"id":"BVQUNzfHgMLO"}},{"cell_type":"code","source":["linear = MyLinear(3,2)\n","\n","y = linear(x)"],"metadata":{"id":"4x746cCmgLSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 여기에서 즁요한 점은 forward 함수를 따로 호출하지 않고 객체명에 바로 괄호를 열어 텐서 x를 인수로 넘겨주었다는 것이다. 이처럼 nn.Module의 상속받은 객체는 __call__ 함수와 forward가 매핑되어 있어서 forward를 직접 부를 필요가 없습니다.\n","\n","- forward 호출 앞뒤로 추가적으로 호출하는 함수가 파이토치 내부에 따로 있기 때문에 사용자가 직접 forward 함수를 호출한느 것을 권장하지 않습니다.\n","\n","- 여기까지 nn.Module을 상속받아 선형 계층을 구성하여 보았습니다. 하지만 이 방법도 아직 제대로 된 방법은 아니다. 물론 파이토치 입장에서 MyLinear라는 클래스의 계층으로 인식하고 계산도 수행하지만 내부에 학습할 수 있는 파라미터는 없는 것으로 인식하기 때문입니다. 예를 들어 다음 코들를 실행하면 아무것도 출력되지 않는다."],"metadata":{"id":"yLG69vtEgsn5"}},{"cell_type":"code","source":["for p in linear.parameters():\n","    print(p)"],"metadata":{"id":"GpptVTQKhtlu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 3. 올바른 방법 : nn.Parameter 활용하기\n","\n","제대로 된 방법은 W와 b를 파이토치에서 학습이 가능하도록 인식할 수 있는 파라미터로 만들어주어야 하는데 torch.nn.parameter 클래스를 활용하면 된다."],"metadata":{"id":"f6e_7N4lh0-t"}},{"cell_type":"code","source":["class MyLinear(nn.Module):\n","\n","    def __init__(self, input_dim=3, output_dim=2):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        super().__init__()\n","\n","        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim))\n","        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n","\n","    # You should override 'forward' method to implement detail.\n","    # The input argument and outputs can be desigend as you wish\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, imput_dim)\n","        y = torch.matmul(x, self.W) + self.b\n","        # |y| = (batch_size, imput_dim) * (input_dim, output_dim) = (batch_size, output_dim)\n","\n","        return y"],"metadata":{"id":"F6ig1DBriFA0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["linear = MyLinear(3,2)\n","\n","y = linear(x)"],"metadata":{"id":"C0fC4RxJi55x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for p in linear.parameters():\n","    print(p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFmtmjAJiVjm","executionInfo":{"status":"ok","timestamp":1671539967260,"user_tz":-540,"elapsed":306,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"1b1c2fac-dfe1-4482-e2c6-02ce0864a6be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[1.1820e-34, 0.0000e+00],\n","        [6.4460e-44, 1.1210e-43],\n","        [1.3593e-43, 1.5975e-43]], requires_grad=True)\n","Parameter containing:\n","tensor([1.1820e-34, 0.0000e+00], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["### 4.nn.Linear 활용하기\n","\n","- 지금까지 사용한 복잡한 방법 말고는 torch.nn에 미리 정의된 선형 계층을 불러다 사용하면 매우 간단하다. 다음 코드는 nn.Linear를 통해 선형 계층을 활용한 모습이다."],"metadata":{"id":"sbhct0Edi-y2"}},{"cell_type":"code","source":["linear = nn.Linear(3,2)\n","\n","y = linear(x)\n","\n","for p in linear.parameters():\n","    print(p)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzJok6lWjTKj","executionInfo":{"status":"ok","timestamp":1671540099846,"user_tz":-540,"elapsed":289,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"de94806a-de1d-442b-ee0a-bc94319e4536"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Parameter containing:\n","tensor([[ 0.3459, -0.3852,  0.1171],\n","        [ 0.3976, -0.0546,  0.1626]], requires_grad=True)\n","Parameter containing:\n","tensor([0.3395, 0.2670], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["- 또한 앞서 말한 대로 nn.Module을 상속받아 정의한 나만의 계층 클래스는 내부의 nn.Module 하위 클래스를 소유할 수 있다."],"metadata":{"id":"vb9Ovkodjg2m"}},{"cell_type":"code","source":["class MyLinear(nn.Module):\n","\n","    def __init__(self, input_dim=3, output_dim=2):\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","        super().__init__()\n","\n","        self.linear = nn.Linear(input_dim, output_dim)\n","\n","    def forward(self, x):\n","        # |x| = (batch_size, input_dim)\n","        y = self.linear(x)\n","        # |y| = (batch_size, output_dim)\n","\n","        return y"],"metadata":{"id":"-T_p8WTpj16P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 앞의 코드는 nn.Module을 상속받아 MyLinear 클래스를 정의하고 있는데, __init__ 함수 내부에는 nn.Linear를 선언하여 self.linear에 저장하는 모습을 보여주고 있다. 그리고 forward 함수에서는 self.linear에 텐서 x를 통과시킨다. 즉, 이 코드도 선형 계층을 구현한 것이라 볼 수 있다."],"metadata":{"id":"SAgctSaNkYAS"}},{"cell_type":"markdown","source":["### 4. GPU사용하기\n","\n","#### 1. Cuda 함수\n","- cuda란? 엔비디아 gpu에서 수행하는 병렬 연산을 프로그래밍 언어를 통해 구현할 수 있도록 하는 기술입니다."],"metadata":{"id":"raw2WBjZkX5x"}},{"cell_type":"code","source":["import torch\n","\n","x = torch.cuda.FloatTensor(2,2)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I3LRwbyvU-UA","executionInfo":{"status":"ok","timestamp":1672643707197,"user_tz":-540,"elapsed":7510,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"2936de9d-f213-4c24-9c50-78d75d8ade15"},"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.]], device='cuda:0')"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","source":["cuda: 뒤에 붙은 숫자는 GPU 디바이스의 인덱스를 의미합니다. 즉, 첫번째 디바이스인 0번 GPU를 의미한다. 앞선 방법 외 텐서의 cuda 함수를 통해 CPU 메모리 상에 선언된 텐서를 GPU로 복사하는 방법도 존재"],"metadata":{"id":"k3Jz-HJ3VY_C"}},{"cell_type":"code","source":["x = torch.FloatTensor(2,2)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PGtOtm4VvY8","executionInfo":{"status":"ok","timestamp":1672643823732,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"48793144-2396-401c-cbe5-1328a352bfeb"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[11886912.,        0.],\n","        [       0.,        0.]])"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["x = x.cuda()\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHqA_cvnV2No","executionInfo":{"status":"ok","timestamp":1672643837486,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"d64fb515-c06d-41a3-eebd-c6acc9e1e4cb"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[11886912.,        0.],\n","        [       0.,        0.]], device='cuda:0')"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["x = x.cuda(device=1) # 만약 2번째 gpu가 없다면 오류가 아래처럼 오류가 뜬다"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"id":"2ZF-RINoV6Gl","executionInfo":{"status":"error","timestamp":1672643912807,"user_tz":-540,"elapsed":336,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"48723257-068d-4475-827b-060929b88634"},"execution_count":4,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-a0f8759ec8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."]}]},{"cell_type":"markdown","source":["cuda 함수는 텐서뿐만 아니라 nn.Module의 하위 클래스 객체에서도 똑같이 적용할 수 있다."],"metadata":{"id":"FA1qlSr3WXD_"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","layer = nn.Linear(2,2)\n","layer.cuda(0)\n","\n","# 여기에서 주의할 점은 텐서는 cuda 함수를 통해 원하는 디바이스로 복사가 되었지만, nn.Module 하위 클래스 객체의 경우 복사가 아닌 이동이 수행된다는 점이다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aRmrecvyWgMc","executionInfo":{"status":"ok","timestamp":1672644032556,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"043314a1-4b4f-48e3-c955-1f50afc76ed7"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=2, out_features=2, bias=True)"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["#### 2. 서로 다른 장치 간 연산\n","- 서로 다른 장치에 있는 텐서 또는 nn.Module의 하위 클래스 객체끼리는 연산이 불가능하다. cpu와 gpu에 위치한 텐서들끼리 연산이 불가능할 뿐만 아니라 0번 gpu와 1번 gpu 사이의 연산도 불가능하다."],"metadata":{"id":"yCy3gOZcYOgK"}},{"cell_type":"code","source":["x = torch.FloatTensor(2,2)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMCGw51DYnfn","executionInfo":{"status":"ok","timestamp":1672644570740,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"3f4e9a21-5aeb-4ba2-e641-8a4bdc7dbc75"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1887e+07, 0.0000e+00],\n","        [8.5479e-44, 4.4842e-44]])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["x + x.cuda(0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":172},"id":"P7nw4aCDYrjT","executionInfo":{"status":"error","timestamp":1672644582708,"user_tz":-540,"elapsed":6,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"4617bb07-0991-4ed7-fa98-b3445e422cdf"},"execution_count":7,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-124aab04b3ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"]}]},{"cell_type":"markdown","source":["#### 3. cpu 함수"],"metadata":{"id":"37TYkJEsYu1A"}},{"cell_type":"code","source":["x = torch.cuda.FloatTensor(2,2)\n","\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rGUpUFDAYxD7","executionInfo":{"status":"ok","timestamp":1672644618884,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"83e2f275-f1c7-4c0f-ad6d-cbb8fb72261f"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1887e+07, 0.0000e+00],\n","        [8.5479e-44, 4.4842e-44]], device='cuda:0')"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["x = x.cpu()\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZEODx8FY3BD","executionInfo":{"status":"ok","timestamp":1672644629074,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"b2b23143-9a24-4449-e07d-60fca5d009d6"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1887e+07, 0.0000e+00],\n","        [8.5479e-44, 4.4842e-44]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["#### 4. To함수\n","\n","to 함수는 원하는 디바이스의 정보를 담은 객체를 인자로 받아, 함수 자신을 호출한 객체를 해당 디바이스로 복사시킵니다. 디바이스 정보를 담은 torch.device를 통해 생성할 수 있습니다."],"metadata":{"id":"Ip0ExSkNY6fM"}},{"cell_type":"code","source":["cpu_device = torch.device('cpu')\n","gpu_device = torch.device('cuda:0')"],"metadata":{"id":"ZtIzqNJkaInX","executionInfo":{"status":"ok","timestamp":1672644999908,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["x = torch.FloatTensor(2,2)\n","\n","x = x.to(gpu_device)\n","x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KrelwKgXaUBs","executionInfo":{"status":"ok","timestamp":1672645026038,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"60046ffa-541a-4958-e2bf-9acef94fd586"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.1891e+07, 0.0000e+00],\n","        [8.5479e-44, 4.4842e-44]], device='cuda:0')"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["#### 5. Device 속성\n","텐서는 device 속성을 가직 있어 해당 텐서가 위치한 디바이스를 쉽게 파악할 수 있다."],"metadata":{"id":"oCoPHpSZaoe3"}},{"cell_type":"code","source":["x = torch.cuda.FloatTensor(2,2)\n","x.device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bCOogo7ax_Y","executionInfo":{"status":"ok","timestamp":1672645151828,"user_tz":-540,"elapsed":247,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"4805e0f3-f0ba-4613-9b2a-8e02615dce51"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["layer = nn.Linear(2,2)\n","next(layer.parameters()).device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EjFOQapha5Jv","executionInfo":{"status":"ok","timestamp":1672645180711,"user_tz":-540,"elapsed":251,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"b8af6f78-1ccc-47bb-94d7-069e50f5b731"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["## 4.6 마치며\n","\n","### 요약\n","\n","1. 선형 계층\n"," - 선형 계층은 행렬의 곱과 벡터의 덧셈으로 이루어져 있음\n"," - 선형 계층 또한 함수이며 내부의 가중치 파라미터의 값에 따라 출력값이 달라짐\n"," - 선형 계층을 통해 선형 데이터에 대한 관계를 분석하거나 선형 함수를 근사계산할 수 있음\n","\n","2. 선형 계층의 수식\n"," - $y = f(x) = x*W+b$ \n","\n","3. gpu 활용하기\n"," - to 함수를 통해 텐서나 모델을 원하는 gpu로 복사/이동시킬 수 있음\n"," - 텐서끼리의 연산은 같은 장치(디바이스)에서만 가능\n"],"metadata":{"id":"awJsJhTKbC8K"}}]}