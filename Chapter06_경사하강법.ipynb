{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2Ko4jgvpr8p30Mf3nt3sx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 6.1 미분이란\n","\n","### 1. 기울기\n","- 기울기는 x 증가량에 대한 y 증가량으로 정의된다\n","\n","### 2. 극한과 미분\n","- x의 변화량이 매우 작을 때 y의 변화량 또한 구할 수 있을 것이다.\n","- 앞에서는 두 점 사이의 기울기였지만 지금은 함수 $f$ 위의 지점 (x, f(x))에서의 접섭의 기울기라고 볼 수 있다.\n","\n","### 3. 도함수\n","- $$g(x) = \\lim_{h->0}^n \\frac{f(x+h-f(x)}{(x+h)-x}$$\n","- 이때 함수 $f$의 기울기를 함수로 나태난 g를 도함수라고 부르며 함수 $f$를 미분하여 얻을 수 있습니다.\n","\n","### 4. 뉴턴 vs 라이프니츠\n","- 미분 표기법 중 작은 따옴표를 붙여 사용하는 방법은 뉴턴의 표기법이고 델타를 사용하여 분수처럼 표기하는 방법은 라이프니츠의 표기법이다.\n","- 여러 개의 변수가 주어진 상황에서는 라이프니츠 방식의 미분 표기법이 훨씬 더 직관적으로 표현되는 것을 알 수 있다."],"metadata":{"id":"yNEiAiP95v33"}},{"cell_type":"markdown","source":["## 6.2 편미분\n","\n","### 1. 편미분\n","- 함수의 변수 중 하나의 입력 변수에 대해서만 미분을 수행하는 것을 편미분이라고 한다.\n","- 다 변수 함수 $f$에서 특정 지점 (x,y)의 기울기는 각 변수별 미분 결과를 합쳐서 벡터로 나타내게 된다. 1치원 함수에서의 미분 결과는 기울기의 스칼라 값이지만, 다변수 함수에서의 미분 결과는 입력 변수 개수만큼의 차원을 갖는 벡터가 됩니다.\n","\n","### 2. 함수의 입출력 형태\n","\n","벡터를 입력으로 받는 함수\n","- 다변수 함수는 벡터를 입력으로 받는 것이라고 생각할 수있다. 따라서 다음 수식처럼 n차원의 벡터 x를 입력으로 받는 함수라고 볼 수 있다.\n","- $y = f(\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}) = f(x),$\n","- $where X \\in \\mathbb{R}^n$\n","\n","행렬을 입력으로 받는 함수\n","- $y = f\\begin{bmatrix}\n","x_{1,1} & \\cdots & x_{1,m} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","x_{n,1}& \\cdots & x_{n,m}\n","\\end{bmatrix}) = f(X),$\n","- $where X \\in \\mathbb{R}^{n*m}$\n","\n","출력이 벡터 또는 행렬인 함수\n","- 출력이 벡터이거나 행렬인 함수 형태도 생각해 볼 수 있다. 다음 수식은 각각 출력이 n차원의 벡터이거나, n*m 차원의 행렬인 경우를 보여준다.\n","- $y = f(\\begin{bmatrix}y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}) = f(x),$\n","- $where X \\in \\mathbb{R}^n$\n","\n","- $Y = f\\begin{bmatrix}\n","y_{1,1} & \\cdots & y_{1,m} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","y_{n,1}& \\cdots & y_{n,m}\n","\\end{bmatrix}) = f(x),$\n","- $where X \\in \\mathbb{R}^{n*m}$\n","\n","입력과 출력이 벡터인 함수\n","- 입력과 출력이 벡터이거나 행렬인 함수도 생각해볼 수 있을 것입니다. 다음 수식은 입력과 출력이 각각 n차원과 m차원인 함수를 수식으로 표현한 것입니다.\n","- n개의 입력을 받아 m개의 출력을 내보내는 선형 계층도 여기에 속한다.\n","- $y = f(\\begin{bmatrix}y_1 \\\\ \\vdots \\\\ y_n \\end{bmatrix}) = f(x) = \n","f(\\begin{bmatrix}x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}),$\n","- $where X \\in \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$"],"metadata":{"id":"qvSiHoHP7lUA"}},{"cell_type":"markdown","source":["### 3. 행렬의 미분\n","\n","스칼라를 벡터로 미분\n","- ${\\partial f \\over \\partial x} = \\bigtriangledown , x^f = \\begin{bmatrix}\\partial f \\over \\partial x_1 \\\\ \\vdots \\\\ \\partial f \\over \\partial x_n \\end{bmatrix}$\n","- $where x \\in \\mathbb{R}^{n}$\n","\n","스칼라를 행렬로 미분\n","- ${\\partial f \\over \\partial X} = \\bigtriangledown , x^f = \\begin{bmatrix}\n","{\\partial f \\over \\partial x_{1,1}} & \\cdots & {\\partial f \\over \\partial x_{1,m}} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","{\\partial f \\over \\partial x_{n,1}} & \\cdots & {\\partial f \\over \\partial x_{n,m}} \\end{bmatrix}$\n","- $where x \\in \\mathbb{R}^{n*m}$\n","\n","벡터를 스칼라로 미분\n","- ${\\partial f \\over \\partial x} = \\begin{bmatrix}\\partial f_{1} \\over \\partial x & \\cdots & \\partial f_{n} \\over \\partial x \\end{bmatrix}$\n","- $where x \\in \\mathbb{R}^{n}$\n","\n","벡터를 벡터로 미분\n","- ${\\partial f \\over \\partial x} = \\begin{bmatrix}\\partial f \\over \\partial x_1 \\\\ \\vdots \\\\ \\partial f \\over \\partial x_n \\end{bmatrix} = \\begin{bmatrix}\\partial f_{1} \\over \\partial x & \\cdots & \\partial f_{n} \\over \\partial x \\end{bmatrix} = \\begin{bmatrix}\n","{\\partial f_{1} \\over \\partial x_{1}} & \\cdots & {\\partial f_{m}ㅡ \\over \\partial x_{1}} \\\\\n","\\vdots & \\ddots & \\vdots \\\\\n","{\\partial f_{1} \\over \\partial x_{n}} & \\cdots & {\\partial f_{m} \\over \\partial x_{n}} \\end{bmatrix}$\n","- $where x \\in \\mathbb{R}^{n} and f(x) \\in \\mathbb{R}^{m}$\n"],"metadata":{"id":"pKD8sBjkA4hw"}},{"cell_type":"markdown","source":["## 6.3 경사하강법\n","- 효율적으로 손실 함수의 출력을 최소로 만드는 입력을 찾기 위한 방법\n","\n","### 1. 1차원에서의 경사하강법\n","- 경사하강법은 미분 가능한 복잡한 함수가 있을 때 해당 함수의 최소점을 찾기 위한 방법이다.\n","\n","### 2. 경사하강법의 수식 표현\n","- $x \\leftarrow x - \\boldsymbol{\\eta}*{df \\over dx}$\n","- $where y = f(x)$\n","- 함수 $f$의 출력값 y를 미분하면 각 지점에 대한 기울기를 구할 수 있다.\n","- 그다음 움직여야 하는 다음 지점의 위치는 현재 지점의 기울기를 빼주면 구할 수 있다.\n","\n","### 3. 전역 최소점과 지역 최소점\n","- 손실 함수를 대입하면 손실 값이 최소가 되는 모델의 가중치 파라미터가 되는 점을 전역 최소점이라고 한다.\n","- 이와 반대로 전역 최소점은 아니지만 하강 구간에 존재하는 가장 낮은 부분을 지역 최소적이라고 한다.\n","\n","### 4. 다차원으로 확장\n","- 앞 예제들은 1차원 함수 위에서의 경사하강법에 대한 언급이었다. 이제부터는 2차원 벡터를 입력으로 받는 함수에 대하여 생각해본다.\n","- $y = f(x) = f(\\begin{bmatrix}x_1 \\\\ x_n \\end{bmatrix}), where \\in \\mathbb{R}^{2}$\n","- 결과저으로 우리는 다음 x를 $x \\leftarrow x - \\boldsymbol{\\eta}*\\nabla x_{f}$ 다음과 같이 업데이트할 수 있습니다.\n","\n","### 5. 모델 가중치 파라미터 확장\n","- 모델 함수의 가중치 파라미터에 대해서도 문제를 확장해볼 수 있습니다. 모델 함수가 선형 계층 함수로 구성되어 있다고 가정한다면 가중치 파라미터를 다음과 같이 정의할 수 있을 것이다.\n","\n","- $f_{𝛩}(x) = x*W + b, and \\ θ = {W, b}$,\n","- $where W \\in \\mathbb{R}^{n*m} and \\ b \\in \\mathbb{R}^{m}$\n","- 수식에서 보이듯이 n * (m+1) 차원의 공간에서 최적의 가중치 파라미터를 찾아야한다. 이를 위해 각 가중치 파라미터 W와 b로 손실 함수를 미분하여 그레디언트를 구하고 파라미터들을 업데이트 할 수 있다.\n","\n","딥러닝에서 지역 최소점 문제\n","- 여러 계층을 쌓는 심층신경망을 모델로 활용한다면 가중치 파라미터는 커질 수 밖에 없다. 다만 높은 차원에서 지역 최소점이 큰 문제가 되지 않는다는 것이 학계의 중론이다. 지역 최소점에 빠지더라고 그 점은 아마도 경험적으로 전역 최소점의 근방에 위치할 것이라는 말도 존재한다. 이것은 지역 최소점을 구성하기 위한 조건들이 높은 차원에서는 만족되기 어렵기 때문이다."],"metadata":{"id":"QPDRLGGeFkVr"}},{"cell_type":"markdown","source":["## 6.4 학습률에 따른 성질\n","- $Θ ← - η*{\\partial \\mathcal{L(Θ)} \\over dx}$\n","- 학습률은 η(에타) 기호로 표시되며 보통 0에서 1사이의 값을 가진다. 기울기 벡터인 그레디언트에 곱해지는 된다. 따라서 가중치 파라미터가 업데이트 될 때 기울기의 값을 얼마큼 반영할 것인지 설정하는 것이라고 볼 수 있다.  \n","- 만약 학습률이 너쿠 커서 손실 값이 발산하는 상황이라면 학습률을 50% 또는 10%로 줄려주는 것이 좋다"],"metadata":{"id":"_EuaUHOwRNuc"}},{"cell_type":"markdown","source":["## 6.5 경사하강법 구현\n","- 경사하강법을 통해 랜덤하게 생성한 텐서가 특정 텐서 값을 근사 계산하도록 파이토치를 구현해본다. 여기에서 함수의 출력은 목표 텐서와 랜덤 텐서 사이의 차이가 될 것이도 함수의 입력은 랜덤 생성한 텐서의 현재 값이 된다. 따라서 랜덤 생성한 텐의 값을 경사하강법을 활용하여 이리저리 바꿔가며 함수의 출력값을 최소화해보도록 하겠다."],"metadata":{"id":"gRamtA7kTd2W"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"Uknw5Ij32Nwi","executionInfo":{"status":"ok","timestamp":1674102994777,"user_tz":-540,"elapsed":4363,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F"]},{"cell_type":"code","source":["target = torch.FloatTensor([[.1, .2, .3],\n","                            [.4, .5, .6],\n","                            [.7, .8, .9]])"],"metadata":{"id":"w_LGGatdUDyF","executionInfo":{"status":"ok","timestamp":1674103044825,"user_tz":-540,"elapsed":270,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# requires_grad 속성 True\n","x = torch.rand_like(target)\n","\n","x.requires_grad = True\n","\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fzxwvdEgUVVN","executionInfo":{"status":"ok","timestamp":1674103127806,"user_tz":-540,"elapsed":588,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"408e8181-ec17-4a08-b1d5-012253dfabb2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.5585, 0.2163, 0.8081],\n","        [0.1720, 0.2202, 0.3436],\n","        [0.4047, 0.6718, 0.8780]], requires_grad=True)\n"]}]},{"cell_type":"code","source":["# 텐서의 손실 값 계산\n","\n","loss = F.mse_loss(x, target)\n","\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xSDVXyNrUpID","executionInfo":{"status":"ok","timestamp":1674103154496,"user_tz":-540,"elapsed":4,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"ea384d1c-277b-4579-a1a5-ac8c1479039b"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.0854, grad_fn=<MseLossBackward0>)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# while문을 사용한 경사 하강법\n","\n","threshold = 1e-5\n","learning_rate = 1.\n","iter_cnt = 0\n","\n","while loss > threshold:\n","    iter_cnt += 1\n","    loss.backward() # 기울기 계산\n","\n","    x = x - learning_rate * x.grad\n","\n","    x.detach_()\n","    x.requires_grad_(True)\n","\n","    loss = F.mse_loss(x, target)\n","\n","    print('%d-th Loss: %.4e' % (iter_cnt, loss))\n","    print(x)"],"metadata":{"id":"2yJBRB2aUvgi","executionInfo":{"status":"ok","timestamp":1674103389831,"user_tz":-540,"elapsed":1,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## 6.6 파이토치 오토그래드 소개\n","\n","- 파이토치에는 자동 미분 기능을 제공한다. 토치는 requires_grad 속성이 True인 텐서의 연산을 추적하기 위한 계산 그래프를 구축하고, backward 함수가 호출되면 이 그래프를 따라 미분을 자동으로 수행한다."],"metadata":{"id":"13mJvVrbVqDT"}},{"cell_type":"code","source":["x = torch.FloatTensor([[1,2],\n","                       [3,4]]).requires_grad_(True)"],"metadata":{"id":"-lv5e0tYWYgn","executionInfo":{"status":"ok","timestamp":1674103903755,"user_tz":-540,"elapsed":1,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["x1 = x + 2\n","print(x1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DkSMDV2sWga7","executionInfo":{"status":"ok","timestamp":1674103931035,"user_tz":-540,"elapsed":287,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"0847dcfe-57a8-4b59-8d40-04443a11d1dc"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3., 4.],\n","        [5., 6.]], grad_fn=<AddBackward0>)\n"]}]},{"cell_type":"code","source":["x2 = x - 2\n","print(x2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oDu9ZmQ2XmDU","executionInfo":{"status":"ok","timestamp":1674103934507,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"496b6860-691e-4631-9891-314f11e183dc"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-1.,  0.],\n","        [ 1.,  2.]], grad_fn=<SubBackward0>)\n"]}]},{"cell_type":"code","source":["x3 = x1 * x2\n","print(x3)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OE1TJMhWlLi","executionInfo":{"status":"ok","timestamp":1674103937101,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"4c5b07b4-6c75-4877-eb5b-5e6d8d21a39b"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[-3.,  0.],\n","        [ 5., 12.]], grad_fn=<MulBackward0>)\n"]}]},{"cell_type":"code","source":["y = x3.sum()\n","print(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-kxFDOWNXMxO","executionInfo":{"status":"ok","timestamp":1674103939215,"user_tz":-540,"elapsed":280,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"a782f7d1-6e1f-4060-82fd-2b6119426807"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(14., grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"markdown","source":["코드의 결과를 보면 모두 grad_fn이라는 속성을 가진다. 예를 들어 텐서 x1이 덧셈 연산의 결과물이기 때문에 x1의 grad_fn 속성은 AddBackward0이다. 텐서 y는 sum 함수를 쓰므로 스칼라 값이 되었다. 그럼 여기에서 backward 함수를 호출해본다."],"metadata":{"id":"mUnZgqupWq3o"}},{"cell_type":"code","source":["y.backward()"],"metadata":{"id":"GSdOHTQnXXnn","executionInfo":{"status":"ok","timestamp":1674103941632,"user_tz":-540,"elapsed":2,"user":{"displayName":"GR Son","userId":"00689596548911390893"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["x, x1, x2, x3, y 모두 grad 속성에 저장되어있다. 수식으로 나타내면 다음과 같다.\n","\n","- $x = \\begin{bmatrix}x_{1,1} & x_{1,2} & \\\\ x_{2,1} & x_{2,2} \\end{bmatrix}$\n","- $x_1 = x + 2$, $x_2 = x - 2$, $x_3 = x_1 * x_2$\n","- $y = sum(x_3) = x_{3(1,1)}, x_{3(1,2)}, x_{3(2,1)}, x_{3(2,2)}$ 이렇게 구한 y를 다시 x로 미분하면 ${dy \\over dx} = 2x$\n"],"metadata":{"id":"YNOywi_SXvkh"}},{"cell_type":"code","source":["print(x.grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nBjxbX2OXf5j","executionInfo":{"status":"ok","timestamp":1674103943604,"user_tz":-540,"elapsed":3,"user":{"displayName":"GR Son","userId":"00689596548911390893"}},"outputId":"31a383bc-06d6-4971-eff2-2dcf37babbd6"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[2., 4.],\n","        [6., 8.]])\n"]}]},{"cell_type":"markdown","source":["## 6.7 마치며\n","\n","### 요약\n","\n","1. 경사하강법이 필요한 이유\n"," - 신경망의 내부 가중치 파라미터를 잘 조절하여 원하는 함수를 근사계산하고 싶을 때 할용할 수 있다.\n"," - 손실 함수를 통해 더 좋은 가중치를 파라미터를 선택할 수 있다.\n"," - 효율적으로 가중치 파라미터를 찾고 싶을 때 사용\n"," - 손실 값을 가중치 파라미터로 미분하여 그래디언트 벡터를 구하고 반대 방향으로 가중치 파라미터를 업데이트하면 점진적으로 더 낮은 손실 값을 갖는 가중치 파라미터를 구할 수 있다.\n","\n","2. 학습률\n"," - 학습률이 너무 클 경우 : 파라미터 업데이트 과정에서 손실 값이 발산할 수 있다.\n"," - 학습률이 너무 작을 경우 : 파라미터가 너무 조금씩 바뀌어서 학습이 느려진다.\n","\n","3. 오토그래드\n"," - 파이토치는 오토그래드 알고리즘을 통해 미분을 자동으로 수행할 수 있다.\n"," - 스칼라 값에 backward 함수를 호출하여 자동 미분을 수행\n"," - 텐서간의 연산마다 계산 그래프를 자동으로 생성하고 나중에 미분할 때 활용"],"metadata":{"id":"lJDsXaFbZGGD"}}]}