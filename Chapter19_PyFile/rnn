{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOB6GTCqmA+bQAqSFFB0kf1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"cnwFbnfxm8WM"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class SequenceClassifier(nn.Module):\n","\n","    def __init__(\n","        self,\n","        input_size,\n","        hidden_size,\n","        output_size,\n","        n_layers=4,\n","        dropout_p=.2,\n","    ):\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.dropout_p = dropout_p\n","\n","        super().__init__()\n","\n","        # LSTM 클래스 객체를 생성하여 RNN에 할당\n","        self.rnn = nn.LSTM(\n","            input_size = input_size,,\n","            hidden_size = hidden_size,\n","            num_layers = n_layers, # 다계층\n","            batch_first = True, # batch_firstfmf True로 설정하는 이유는는 아래 참고\n","            dropout = dropout_p, # LSTM 내부 계층 사이사이에 드롭아웃을 넣어주도록한다.\n","            bidirectional = True, # 양방향\n","        )\n","        # self.layers 변수에 하래 객체를 할당한다.\n","        self.layers = nn.Sequential( \n","            nn.ReLU(), # 활성화 함수\n","            nn.BatchNorm1d(hidden_size * 2), # 배치정규화\n","            nn.Linear(hidden_size * 2, output_size), # 선형 계층\n","            nn.LogSoftmax(dim=-1) # 로그 소프트맥스 계층\n","        )\n","\n","    # init 메서드에서 생성된 객체들을 활용하여 연산을 수행\n","    # 다대일 형태를 문제를 다루므로 출력 텐서만 활용\n","    def forward(self, x): \n","        # |x| = (batch_size, h, w)\n","        z, _ = self.rnn(x) # 입력 텐서 x 삽입\n","        # |z| = (batch_size, h, hidden_size * 2)\n","        z = z[:, -1] # 출력 텐서만 활용\n","        # |z| = (batch_size, hidden_size * 2)\n","        y = self.layers(z)\n","        # |y| = (batch_size, output_size)\n","\n","        return y"]}]}